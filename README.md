## Частина 1. Базовий функціонал: Скрейп категорії на Rozetka

Завдання

* Зробити скрейп категорії "[Монітори](https://hard.rozetka.com.ua/ua/monitors/c80089/)" на сайті rozetka.ua
* Зібрати основні дані про товари:

  * назва (title)
  * ціна (price)
  * посилання на товар (url)
  * артикул / SKU (якщо є)
  * наявність (availability)

Очікуваний результат:

* Визначити чи можна спарсити ріквестами. Якщо так, то які обмеження, чи вдається всі товари витягнути ріквестами.
* CSV/JSON файл з коректно зібраними даними.
* Код на Python (бажано з використанням requests + BeautifulSoup/lxml або httpx + parsel).
* Коротка документація: як запустити скрипт і який формат вихідних даних.
* Короткий результат дослідження сайту - який захист, яким методом вдалось/не вдалось витягати дані, кількість товарів які вдалось чи вдалось би витягнути, які ресурси - проксі + машинний час треба щоб спарсити категорію.

## Встановлення бібліотек

pip install -r requirements.txt

## Встановлення параметріа в .env

### `max_workers=10`

* **Кількість потоків** для паралельної обробки
* Більше потоків = швидше, але більше навантаження на сервер
* Рекомендовано: 5-15 залежно від потужності ПК

### `batch_size=60`

* **Розмір батча** товарів за один запит
* Rozetka API дозволяє до 60 товарів в одному запиті
* Не змінювати без потреби

### `max_retries=1`

* **Кількість повторних спроб** при помилках запитів
* 1 = одна спроба, 3 = три спроби з затримками
* Рекомендовано: 1-3

### Затримки та таймаути

### `min_delay=0.05` / `max_delay=0.5`

* **Випадкові затримки** між запитами (у секундах)
* `min_delay` - мінімальна затримка
* `max_delay` - максимальна затримка
* Захист від блокування, імітація людської поведінки

### `request_timeout=30`

* **Таймаут запитів** до API (у секундах)
* Через скільки вважати запит невдалим

### Фільтрація та сортування

### `filters=["Максимальна роздільна здатність дисплея"]`

* **Список фільтрів** для поглибленого парсингу
* Доступні фільтри залежать від категорії
* `[]` або `None` - парсинг без фільтрів

### `sort_list=["rank", "novelty", "cheap", "expensive"]`

* **Способи сортування** для збору більшої кількості товарів
* Доступні значення: `rank`, `novelty`, `cheap`, `expensive`
* `[]` або `None` - парсинг без сортування
* `[]` означає що буде просто парсинг всіх сторінок 

### `default_parse=True`

* **Додатковий парсинг** без фільтрів
* `True` - збирати товари з фільтрами та без фільтрів
* `False` - тільки з вказаними фільтрами

### Збереження даних

### `save_data=csv`

* **Формат збереження** результатів
* `csv` - тільки CSV файли
* `json` - тільки JSON файли
* `both` - обидва формати

## Встановлення лінків для парсингу

В файл **category_to_parse.txt** треба вставити на кожний новій рядок посилання на категорію

### Запуск

python -m src
